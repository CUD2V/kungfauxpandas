@misc{hippaviol,
    author    = {{American Medical Association}},
    title     = {{HIPAA} violations \& enforcement},
    howpublished = {\url{https://www.ama-assn.org/practice-management/hipaa-violations-enforcement}},
    note = {Accessed: 2018-05-05}
}

@misc{hippapro,
    author    = {{United States Department of Health and Human Services}},
    title     = {HIPAA for professionals},
    howpublished = {\url{https://www.hhs.gov/hipaa/for-professionals/index.html}},
    note = {Accessed: 2018-05-08}
}

@misc{natmethods,
    author    = {2018 Macmillan Publishers Limited, part of Springer Nature},
    title     = {Availability of data, material and methods},
    howpublished = {\url{https://www.nature.com/authors/policies/availability.html}},
    note = {Accessed: 2018-05-09}
} 

@misc{gdpr,
    title     = {General data protection regulation},
    howpublished = {\url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=uriserv:OJ.L_.2016.119.01.0001.01.ENG&toc=OJ:L:2016:119:TOC}},
    note = {Accessed: 2018-05-25}
}

@article{beaulieu-jones_privacy-preserving_2017,
	title = {Privacy-preserving generative deep neural networks support clinical data sharing},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/11/15/159756},
	doi = {10.1101/159756},
	abstract = {Though it is widely recognized that data sharing enables faster scientific progress, the sensible need to protect participant privacy hampers this practice in medicine. We train deep neural networks that generate synthetic subjects closely resembling study participants. Using the {SPRINT} trial as an example, we show that machine-learning models built from simulated participants generalize to the original dataset. We incorporate differential privacy, which offers strong guarantees on the likelihood that a subject could be identified as a member of the trial. Investigators who have compiled a dataset can use our method to provide a freely accessible public version that enables other scientists to perform discovery-oriented analyses. Generated data can be released alongside analytical code to enable fully reproducible workflows, even when privacy is a concern. By addressing data sharing challenges, deep neural networks can facilitate the rigorous and reproducible investigation of clinical datasets.},
	pages = {159756},
	journaltitle = {{bioRxiv}},
	author = {Beaulieu-Jones, Brett K. and Wu, Zhiwei Steven and Williams, Chris and Byrd, James Brian and Greene, Casey S.},
	urldate = {2018-03-02},
	date = {2017-11-15},
	langid = {english}
}

@inproceedings{patki_synthetic_2016,
	title = {The synthetic data vault},
	doi = {10.1109/DSAA.2016.49},
	abstract = {The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault ({SDV}), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name {SDV}. When implementing the {SDV}, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The {SDV} iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the {SDV} to synthesize data by sampling from any part of the database. After building the {SDV}, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the {SDV} is a viable solution for synthetic data generation.},
	eventtitle = {2016 {IEEE} International Conference on Data Science and Advanced Analytics ({DSAA})},
	pages = {399--410},
	booktitle = {2016 {IEEE} International Conference on Data Science and Advanced Analytics ({DSAA})},
	author = {Patki, N. and Wedge, R. and Veeramachaneni, K.},
	date = {2016-10},
	year = {2016},
	keywords = {Computational modeling, crowd sourcing, data analysis, Data models, data science, Databases, generative model, Hidden Markov models, multivariate modelling, Numerical models, predictive model, predictive modeling, Predictive models, relational database, relational databases, {SDV}, Synthetic data generation, synthetic data vault}
}

@article{choi_generating_2017,
	title = {Generating multi-label discrete patient records using generative adversarial networks},
	url = {http://arxiv.org/abs/1703.06490},
	abstract = {Access to electronic health record ({EHR}) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of {EHR} data. Sharing synthetic {EHR} data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network ({medGAN}), to generate realistic synthetic patient records. Based on input real patient records, {medGAN} can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that {medGAN} generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using {medGAN}.},
	journal = {{arXiv}:1703.06490 [cs]},
	author = {Choi, Edward and Biswal, Siddharth and Malin, Bradley and Duke, Jon and Stewart, Walter F. and Sun, Jimeng},
	urldate = {2018-03-02},
	date = {2017-03-19},
	year = {2017},
	eprinttype = {arxiv},
	eprint = {1703.06490},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{miotto_deep_2016,
	title = {Deep patient: An unsupervised representation to predict the future of patients from the electronic health records},
	volume = {6},
	rights = {2016 Nature Publishing Group},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep26094},
	doi = {10.1038/srep26094},
	shorttitle = {Deep Patient},
	abstract = {Secondary use of electronic health records ({EHRs}) promises to advance clinical research and better inform clinical decision making. Challenges in summarizing and representing patient data prevent widespread practice of predictive modeling using {EHRs}. Here we present a novel unsupervised deep feature learning method to derive a general-purpose patient representation from {EHR} data that facilitates clinical predictive modeling. In particular, a three-layer stack of denoising autoencoders was used to capture hierarchical regularities and dependencies in the aggregated {EHRs} of about 700,000 patients from the Mount Sinai data warehouse. The result is a representation we name “deep patient”. We evaluated this representation as broadly predictive of health states by assessing the probability of patients to develop various diseases. We performed evaluation using 76,214 test patients comprising 78 diseases from diverse clinical domains and temporal windows. Our results significantly outperformed those achieved using representations based on raw {EHR} data and alternative feature learning strategies. Prediction performance for severe diabetes, schizophrenia, and various cancers were among the top performing. These findings indicate that deep learning applied to {EHRs} can derive patient representations that offer improved clinical predictions, and could provide a machine learning framework for augmenting clinical decision systems.},
	pages = {26094},
	journaltitle = {Scientific Reports},
	author = {Miotto, Riccardo and Li, Li and Kidd, Brian A. and Dudley, Joel T.},
	urldate = {2018-03-02},
	date = {2016-05-17},
	langid = {english}
}


@article{walonoski_synthea_2018,
	title = {Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record},
	volume = {25},
	issn = {1067-5027},
	shorttitle = {Synthea},
	url = {https://academic.oup.com/jamia/article/25/3/230/4098271},
	doi = {10.1093/jamia/ocx079},
	abstract = {ObjectiveOur objective is to create a source of synthetic electronic health records that is readily available; suited to industrial, innovation, research, and educational uses; and free of legal, privacy, security, and intellectual property restrictions.Materials and MethodsWe developed Synthea, an open-source software package that simulates the lifespans of synthetic patients, modeling the 10 most frequent reasons for primary care encounters and the 10 chronic conditions with the highest morbidity in the United States.ResultsSynthea adheres to a previously developed conceptual framework, scales via open-source deployment on the Internet, and may be extended with additional disease and treatment modules developed by its user community. One million synthetic patient records are now freely available online, encoded in standard formats (eg, Health Level-7 [HL7] Fast Healthcare Interoperability Resources [FHIR] and Consolidated-Clinical Document Architecture), and accessible through an HL7 FHIR application program interface.DiscussionHealth care lags other industries in information technology, data exchange, and interoperability. The lack of freely distributable health records has long hindered innovation in health care. Approaches and tools are available to inexpensively generate synthetic health records at scale without accidental disclosure risk, lowering current barriers to entry for promising early-stage developments. By engaging a growing community of users, the synthetic data generated will become increasingly comprehensive, detailed, and realistic over time.ConclusionSynthetic patients can be simulated with models of disease progression and corresponding standards of care to produce risk-free realistic synthetic health care records at scale.},
	language = {en},
	number = {3},
	urldate = {2018-03-02},
	journal = {Journal of the American Medical Informatics Association},
	author = {Walonoski, Jason and Kramer, Mark and Nichols, Joseph and Quina, Andre and Moesel, Chris and Hall, Dylan and Duffett, Carlton and Dube, Kudakwashe and Gallagher, Thomas and McLachlan, Scott},
	month = mar,
	year = {2018},
	pages = {230--238}
}

@inproceedings{gray_quickly_1994,
	address = {New York, NY, USA},
	series = {{SIGMOD} '94},
	title = {Quickly generating billion-record synthetic databases},
	isbn = {978-0-89791-639-4},
	url = {http://doi.acm.org/10.1145/191839.191886},
	doi = {10.1145/191839.191886},
	abstract = {Evaluating database system performance often requires generating synthetic databases—ones having certain statistical properties but filled with dummy information. When evaluating different database designs, it is often necessary to generate several databases and evaluate each design. As database sizes grow to terabytes, generation often takes longer than evaluation. This paper presents several database generation techniques. In particular it discusses: (1) Parallelism to get generation speedup and scaleup. (2) Congruential generators to get dense unique uniform distributions. (3) Special-case discrete logarithms to generate indices concurrent to the base table generation. (4) Modification of (2) to get exponential, normal, and self-similar distributions.The discussion is in terms of generating billion-record SQL databases using C programs running on a shared-nothing computer system consisting of a hundred processors, with a thousand discs. The ideas apply to smaller databases, but large databases present the more difficult problems.},
	urldate = {2018-05-09},
	booktitle = {Proceedings of the 1994 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Gray, Jim and Sundaresan, Prakash and Englert, Susanne and Baclawski, Ken and Weinberger, Peter J.},
	year = {1994},
	pages = {243--252}
}

@article{sweeney_2002,
	title = {k-Anonymity: A model for protecting privacy},
	volume = {10},
	issn = {0218-4885},
	shorttitle = {k-{ANONYMITY}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218488502001648},
	doi = {10.1142/S0218488502001648},
	abstract = {Consider a data holder, such as a hospital or a bank, that has a privately held  collection of person-specific, field structured data. Suppose the data holder  wants to share a version of the data with researchers. How can a data holder release  a version of its private data with scientific guarantees that the individuals who are  the subjects of the data cannot be re-identified while the data remain  practically useful? The solution provided in this paper includes a formal  protection model named k-anonymity and a set of accompanying policies  for deployment. A release provides k-anonymity protection if the  information for each person contained in the release cannot be distinguished from  at least k-1 individuals whose information also appears in the release.  This paper also examines re-identification attacks that can be realized on  releases that adhere to k-anonymity unless accompanying policies  are respected. The k-anonymity protection model is important because  it forms the basis on which the real-world systems known as Datafly,  μ-Argus and k-Similar provide guarantees of privacy  protection.},
	number = {05},
	urldate = {2018-05-09},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Sweeney, Latanya},
	month = oct,
	year = {2002},
	pages = {557--570}
}

@article{gal_data_2014,
	title = {A data recipient centered de-identification method to retain statistical attributes},
	volume = {50},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046414000021},
	doi = {10.1016/j.jbi.2014.01.001},
	abstract = {Privacy has always been a great concern of patients and medical service providers. As a result of the recent advances in information technology and th…},
	language = {en},
	urldate = {2018-05-07},
	journal = {Journal of Biomedical Informatics},
	author = {Gal, Tamas S. and Tucker, Thomas C. and Gangopadhyay, Aryya and Chen, Zhiyuan},
	month = aug,
	year = {2014},
	pages = {32--45}
}

@article{aggarwal_static_2008,
	title = {On static and dynamic methods for condensation-based privacy-preserving data mining},
	volume = {33},
	issn = {0362-5915},
	url = {http://doi.acm.org/10.1145/1331904.1331906},
	doi = {10.1145/1331904.1331906},
	abstract = {In recent years, privacy-preserving data mining has become an important problem because of the large amount of personal data which is tracked by many business applications. In many cases, users are unwilling to provide personal information unless the privacy of sensitive information is guaranteed. In this paper, we propose a new framework for privacy-preserving data mining of multidimensional data. Previous work for privacy-preserving data mining uses a perturbation approach which reconstructs data distributions in order to perform the mining. Such an approach treats each dimension independently and therefore ignores the correlations between the different dimensions. In addition, it requires the development of a new distribution-based algorithm for each data mining problem, since it does not use the multidimensional records, but uses aggregate distributions of the data as input. This leads to a fundamental re-design of data mining algorithms. In this paper, we will develop a new and flexible approach for privacy-preserving data mining that does not require new problem-specific algorithms, since it maps the original data set into a new anonymized data set. These anonymized data closely match the characteristics of the original data including the correlations among the different dimensions. We will show how to extend the method to the case of data streams. We present empirical results illustrating the effectiveness of the method. We also show the efficiency of the method for data streams.},
	number = {1},
	urldate = {2018-05-09},
	journal = {ACM Trans. Database Syst.},
	author = {Aggarwal, Charu C. and Yu, Philip S.},
	month = mar,
	year = {2008},
	keywords = {\textit{k}-anonymity, databases data mining, Privacy},
	pages = {2:1--2:39}
}

@inproceedings{drummond_replicability_2009,
	address = {Montreal, Canada},
	title = {Replicability is not reproducibility: Nor is it good science},
	url = {http://www.csi.uottawa.ca/~cdrummon/pubs/ICMLws09.pdf},
	abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our ﬁeld. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientiﬁc discipline, being able to replicate experiments is paramount. I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important diﬀerences between the two. Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.},
	language = {en},
	booktitle = {Proc. of the {Evaluation} {Methods} for {Machine} {Learning} {Workshop} at the 26th {ICML}},
	author = {Drummond, Chris},
	year = {2009},
	pages = {4}
}

@article{leek_opinion_2015,
	title = {Opinion: {Reproducible} research can still be wrong: {Adopting} a prevention approach},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Opinion},
	url = {http://www.pnas.org/content/112/6/1645},
	doi = {10.1073/pnas.1421412111},
	abstract = {National Academy of Sciences},
	language = {en},
	number = {6},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Leek, Jeffrey T. and Peng, Roger D.},
	month = feb,
	year = {2015},
	pmid = {25670866},
	pages = {1645--1646}
}

@article{peng_reproducible_2011,
	title = {Reproducible research in computational science},
	volume = {334},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3383002/},
	doi = {10.1126/science.1213847},
	abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
	number = {6060},
	urldate = {2018-05-09},
	journal = {Science (New York, N.y.)},
	author = {Peng, Roger D.},
	month = dec,
	year = {2011},
	pmid = {22144613},
	pmcid = {PMC3383002},
	pages = {1226--1227}
}

@article{peng_reproducible_2006,
	title = {Reproducible epidemiologic research},
	volume = {163},
	issn = {0002-9262},
	url = {https://academic.oup.com/aje/article/163/9/783/108733},
	doi = {10.1093/aje/kwj093},
	abstract = {Abstract.  The replication of important findings by multiple independent investigators is fundamental to the accumulation of scientific evidence. Researchers in},
	language = {en},
	number = {9},
	urldate = {2018-05-10},
	journal = {American Journal of Epidemiology},
	author = {Peng, Roger D. and Dominici, Francesca and Zeger, Scott L.},
	month = may,
	year = {2006},
	pages = {783--789}
}

@article{schwab_making_2000,
	title = {Making scientific computations reproducible},
	volume = {2},
	issn = {1521-9615},
	url = {doi.ieeecomputersociety.org/10.1109/5992.881708},
	abstract = {To verify a research paper's computational results, readers typically have to recreate them from scratch. ReDoc is a simple software filing system for authors that lets readers easily reproduce computational results using standardized rules and commands.},
	number = {6},
	urldate = {2018-05-10},
	journal = {Computing in Science \& Engineering},
	author = {Schwab, M. and Karrenbach, M. and Claerbout, J.},
	year = {2000},
	pages = {61--67}
}

@inproceedings{ping17datasynthesizer,
  author = {Ping, Haoyue and Stoyanovich, Julia and Howe, Bill},
  title = {DataSynthesizer: Privacy-preserving synthetic datasets},
  booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
  series = {SSDBM '17},
  year = {2017},
  isbn = {978-1-4503-5282-6},
  location = {Chicago, IL, USA},
  pages = {42:1--42:5},
  articleno = {42},
  numpages = {5},
  url = {http://doi.acm.org/10.1145/3085504.3091117},
  doi = {10.1145/3085504.3091117},
  acmid = {3091117},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {Data Sharing, Differential Privacy, Synthetic Data}
}

@article{Bindschaedler2017,
 author = {Bindschaedler, Vincent and Shokri, Reza and Gunter, Carl A.},
 title = {Plausible deniability for privacy-preserving data synthesis},
 journal = {Proc. VLDB Endow.},
 issue_date = {January 2017},
 volume = {10},
 number = {5},
 month = jan,
 year = {2017},
 issn = {2150-8097},
 pages = {481--492},
 numpages = {12},
 url = {https://doi.org/10.14778/3055540.3055542},
 doi = {10.14778/3055540.3055542},
 acmid = {3055542},
 publisher = {VLDB Endowment},
} 

@book{silverman_density_1986,
	address = {London ; New York},
	series = {Monographs on statistics and applied probability},
	title = {Density estimation for statistics and data analysis},
	isbn = {978-0-412-24620-3},
	number = {26},
	publisher = {Chapman and Hall},
	author = {Silverman, B. W.},
	year = {1986},
	keywords = {Estimation theory}
}

@article{howe_synthetic_2017,
	title = {Synthetic data for social good},
	url = {http://arxiv.org/abs/1710.08874},
	abstract = {Data for good implies unfettered access to data. But data owners must be conservative about how, when, and why they share data or risk violating the trust of the people they aim to help, losing their funding, or breaking the law. Data sharing agreements can help prevent privacy violations, but require a level of specificity that is premature during preliminary discussions, and can take over a year to establish. We consider the generation and use of synthetic data to facilitate ad hoc collaborations involving sensitive data. A good synthetic dataset has two properties: it is representative of the original data, and it provides strong guarantees about privacy. In this paper, we discuss important use cases for synthetic data that challenge the state of the art in privacy-preserving data generation, and describe DataSynthesizer, a dataset generation tool that takes a sensitive dataset as input and generates a structurally and statistically similar synthetic dataset, with strong privacy guarantees, as output. The data owners need not release their data, while potential collaborators can begin developing models and methods with some confidence that their results will work similarly on the real dataset. The distinguishing feature of DataSynthesizer is its usability - in most cases, the data owner need not specify any parameters to start generating and sharing data safely and effectively. The code implementing DataSynthesizer is publicly available on GitHub at https://github.com/DataResponsibly. The work on DataSynthesizer is part of the Data, Responsibly project, where the goal is to operationalize responsibility in data sharing, integration, analysis and use.},
	urldate = {2018-05-14},
	journal = {arXiv:1710.08874 [cs]},
	author = {Howe, Bill and Stoyanovich, Julia and Ping, Haoyue and Herman, Bernease and Gee, Matt},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.08874},
	keywords = {Computer Science - Computers and Society}
}

@misc{naep_2009,
	title = {{NAEP} analysis and scaling - minimum school and student sample sizes for reporting group results},
	url = {https://nces.ed.gov/nationsreportcard/tdw/analysis/summary_rules_minimum.aspx},
	abstract = {Enter overall description here.  Each page however should have individual descriptions.},
	language = {EN},
	author = {{National Center for Education Statistics}},
	month = sep,
	year = {2009}
}

@InProceedings{ mckinney-proc-scipy-2010,
	author    = { Wes McKinney },
	title     = { Data structures for statistical computing in python },
	booktitle = { Proceedings of the 9th Python in Science Conference },
	pages     = { 51 - 56 },
	year      = { 2010 },
	editor    = { St\'efan van der Walt and Jarrod Millman }
}

@techreport{ohm_broken_2009,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Broken promises of privacy: Responding to the surprising failure of anonymization},
	shorttitle = {Broken {Promises} of {Privacy}},
	url = {https://papers.ssrn.com/abstract=1450006},
	abstract = {Computer scientists have recently undermined our faith in the privacy-protecting power of anonymization, the name for techniques for protecting the privacy of individuals in large databases by deleting information like names and social security numbers. These scientists have demonstrated they can often 'reidentify' or 'deanonymize' individuals hidden in anonymized data with astonishing ease. By understanding this research, we will realize we have made a mistake, labored beneath a fundamental misunderstanding, which has assured us much less privacy than we have assumed. This mistake pervades nearly every information privacy law, regulation, and debate, yet regulators and legal scholars have paid it scant attention. We must respond to the surprising failure of anonymization, and this Article provides the tools to do so.},
	language = {en},
	number = {ID 1450006},
	urldate = {2018-06-04},
	institution = {Social Science Research Network},
	author = {Ohm, Paul},
	month = aug,
	year = {2009},
	keywords = {anonymization, Data Protection Directive, deidentification, HIPAA, information privacy, privacy, reidentification}
}


@article{hodge_legal_1999,
	title = {Legal issues concerning electronic health information: Privacy, quality, and liability},
	volume = {282},
	issn = {0098-7484},
	shorttitle = {Legal {Issues} {Concerning} {Electronic} {Health} {Information}},
	url = {https://jamanetwork.com/journals/jama/fullarticle/192004},
	doi = {10.1001/jama.282.15.1466},
	abstract = {Personally identifiable health information about individuals and general medical information is increasingly available in electronic form in health databases and through online networks. The proliferation of electronic data within the modern health information infrastructure presents significant benefits for medical providers and patients, including enhanced patient autonomy, improved clinical treatment, advances in health research and public health surveillance, and modern security techniques. However, it also presents new legal challenges in 3 interconnected areas: privacy of identifiable health information, reliability and quality of health data, and tort-based liability. Protecting health information privacy (by giving individuals control over health data without severely restricting warranted communal uses) directly improves the quality and reliability of health data (by encouraging individual uses of health services and communal uses of data), which diminishes tort-based liabilities (by reducing instances of medical malpractice or privacy invasions through improvements in the delivery of health care services resulting in part from better quality and reliability of clinical and research data). Following an analysis of the interconnectivity of these 3 areas and discussing existing and proposed health information privacy laws, recommendations for legal reform concerning health information privacy are presented. These include (1) recognizing identifiable health information as highly sensitive, (2) providing privacy safeguards based on fair information practices, (3) empowering patients with information and rights to consent to disclosure (4) limiting disclosures of health data absent consent, (5) incorporating industry-wide security protections, (6) establishing a national data protection authority, and (7) providing a national minimal level of privacy protections.},
	language = {en},
	number = {15},
	urldate = {2018-06-04},
	journal = {JAMA},
	author = {James G. Hodge, Jr and Gostin, Lawrence O. and Jacobson, Peter D.},
	month = oct,
	year = {1999},
	pages = {1466--1471}
}

@book{committee_hipaa_privacy_rule_2009,
	address = {Washington (DC)},
	series = {The {National} {Academies} {Collection}: {Reports} funded by {National} {Institutes} of {Health}},
	title = {Beyond the HIPAA privacy rule: Enhancing privacy, improving health through research},
	copyright = {Copyright © 2009, National Academy of Sciences.},
	isbn = {978-0-309-12499-7},
	shorttitle = {Beyond the {HIPAA} {Privacy} {Rule}},
	url = {http://www.ncbi.nlm.nih.gov/books/NBK9578/},
	abstract = {The U.S. Department of Health and Human Services (HHS) developed a set of federal standards for protecting the privacy of personal health information under the Health Insurance Portability and Accountability Act of 1996 (HIPAA). The HIPAA Privacy Rule set forth detailed regulations regarding the types of uses and disclosures of individuals’ personally identifiable health information—called “protected health information”—permitted by “covered entities” (health plans, health care clearinghouses, and health care providers who transmit information in electronic form in connection with transactions for which HHS has adopted standards under HIPAA). A major goal of the HIPAA Privacy Rule is to ensure that individuals’ health information is properly protected while allowing the flow of information needed to promote high-quality health care. The HIPAA Privacy Rule also set out requirements for the conduct of health research. The Institute of Medicine Committee on Health Research and the Privacy of Health Information (the committee) was charged with two principal tasks : (1) to assess whether the HIPAA Privacy Rule is having an impact on the conduct of health research, defined broadly as “a systematic investigation, including research development, testing and evaluation, designed to develop or contributee to generalizable knowledge” ; and (2) to propose recommendations to facilitate the efficient and effective conduct of important health research while maintaining or strengthening the privacy protections of personally identifiable health information.},
	language = {eng},
	urldate = {2018-06-04},
	publisher = {National Academies Press (US)},
	editor = {Nass, Sharyl J. and Levit, Laura A. and Gostin, Lawrence O.},
	year = {2009},
	pmid = {20662116}
}

@misc{w3c_working_group_webservices,
	title = {Web services architecture},
	howpublished = {https://www.w3.org/TR/2004/NOTE-ws-arch-20040211/},
	urldate = {2018-06-05},
	author = {{W3C Working Group}}
}

@article{wilson_best_2014,
	title = {Best {Practices} for {Scientific} {Computing}},
	volume = {12},
	issn = {1545-7885},
	url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745},
	doi = {10.1371/journal.pbio.1001745},
	abstract = {We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.},
	number = {1},
	journal = {PLOS Biology},
	author = {Wilson, Greg and Aruliah, D. A. and Brown, C. Titus and Hong, Neil P. Chue and Davis, Matt and Guy, Richard T. and Haddock, Steven H. D. and Huff, Kathryn D. and Mitchell, Ian M. and Plumbley, Mark D. and Waugh, Ben and White, Ethan P. and Wilson, Paul},
	month = jan,
	year = {2014},
	keywords = {Computer software, Computers, Open source software, Programming languages, Research validity, Scientists, Software development, Software tools},
	pages = {e1001745}
}

@article{merkel_docker,
	title = {Docker: {Lightweight} {Linux} {Containers} for {Consistent} {Development} and {Deployment}},
	volume = {2014},
	issn = {1075-3583},
	shorttitle = {Docker},
	url = {http://dl.acm.org/citation.cfm?id=2600239.2600241},
	abstract = {Docker promises the ability to package applications and their dependencies into lightweight containers that move easily between different distros, start up quickly and are isolated from each other.},
	number = {239},
	urldate = {2018-11-02},
	journal = {Linux J.},
	author = {Merkel, Dirk},
	month = mar,
	year = {2014}
}

@misc{docker_website,
	title = {Docker},
	url = {https://www.docker.com/},
	abstract = {Build, Ship, and Run Any App, Anywhere. Learn more about the only enterprise-ready container platform that enables IT leaders to cost-effectively build and manage their application portfolio at their own pace., Docker},
	language = {en},
	urldate = {2018-11-02},
	journal = {Docker}
}