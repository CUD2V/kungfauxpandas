@misc{hippaviol,
    author    = {American Medical Association},
    title     = {HIPAA Violations \& Enforcement},
    howpublished = {\url{https://www.ama-assn.org/practice-management/hipaa-violations-enforcement}},
    note = {Accessed: 2018-05-05}
}

@misc{hippapro,
    author    = {United States Department of Health and Human Services},
    title     = {HIPAA for Professionals},
    howpublished = {\url{https://www.hhs.gov/hipaa/for-professionals/index.html}},
    note = {Accessed: 2018-05-08}
}

@misc{natmethods,
    author    = {2018 Macmillan Publishers Limited, part of Springer Nature},
    title     = {Availability of data, material and methods},
    howpublished = {\url{https://www.nature.com/authors/policies/availability.html}},
    note = {Accessed: 2018-05-09}
}

@misc{gdpr,
    title     = {General Data Protection Regulation},
    howpublished = {\url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=uriserv:OJ.L_.2016.119.01.0001.01.ENG&toc=OJ:L:2016:119:TOC}},
    note = {Accessed: 2018-05-25}
}

@article{beaulieu-jones_privacy-preserving_2017,
	title = {Privacy-preserving generative deep neural networks support clinical data sharing},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/11/15/159756},
	doi = {10.1101/159756},
	abstract = {Though it is widely recognized that data sharing enables faster scientific progress, the sensible need to protect participant privacy hampers this practice in medicine. We train deep neural networks that generate synthetic subjects closely resembling study participants. Using the {SPRINT} trial as an example, we show that machine-learning models built from simulated participants generalize to the original dataset. We incorporate differential privacy, which offers strong guarantees on the likelihood that a subject could be identified as a member of the trial. Investigators who have compiled a dataset can use our method to provide a freely accessible public version that enables other scientists to perform discovery-oriented analyses. Generated data can be released alongside analytical code to enable fully reproducible workflows, even when privacy is a concern. By addressing data sharing challenges, deep neural networks can facilitate the rigorous and reproducible investigation of clinical datasets.},
	pages = {159756},
	journaltitle = {{bioRxiv}},
	author = {Beaulieu-Jones, Brett K. and Wu, Zhiwei Steven and Williams, Chris and Byrd, James Brian and Greene, Casey S.},
	urldate = {2018-03-02},
	date = {2017-11-15},
	langid = {english},
	file = {Full Text PDF:/Users/seth/Zotero/storage/9SZ9DAFQ/Beaulieu-Jones et al. - 2017 - Privacy-preserving generative deep neural networks.pdf:application/pdf;Snapshot:/Users/seth/Zotero/storage/LRILIJXS/159756.html:text/html}
}

@inproceedings{patki_synthetic_2016,
	title = {The Synthetic Data Vault},
	doi = {10.1109/DSAA.2016.49},
	abstract = {The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault ({SDV}), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name {SDV}. When implementing the {SDV}, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The {SDV} iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the {SDV} to synthesize data by sampling from any part of the database. After building the {SDV}, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the {SDV} is a viable solution for synthetic data generation.},
	eventtitle = {2016 {IEEE} International Conference on Data Science and Advanced Analytics ({DSAA})},
	pages = {399--410},
	booktitle = {2016 {IEEE} International Conference on Data Science and Advanced Analytics ({DSAA})},
	author = {Patki, N. and Wedge, R. and Veeramachaneni, K.},
	date = {2016-10},
	year = {2016},
	keywords = {Computational modeling, crowd sourcing, data analysis, Data models, data science, Databases, generative model, Hidden Markov models, multivariate modelling, Numerical models, predictive model, predictive modeling, Predictive models, relational database, relational databases, {SDV}, Synthetic data generation, synthetic data vault},
	file = {IEEE Xplore Abstract Record:/Users/seth/Zotero/storage/HVBIX2NG/7796926.html:text/html;IEEE Xplore Full Text PDF:/Users/seth/Zotero/storage/VQERGNWM/Patki et al. - 2016 - The Synthetic Data Vault.pdf:application/pdf}
}

@article{choi_generating_2017,
	title = {Generating Multi-label Discrete Patient Records using Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1703.06490},
	abstract = {Access to electronic health record ({EHR}) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of {EHR} data. Sharing synthetic {EHR} data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network ({medGAN}), to generate realistic synthetic patient records. Based on input real patient records, {medGAN} can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that {medGAN} generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using {medGAN}.},
	journal = {{arXiv}:1703.06490 [cs]},
	author = {Choi, Edward and Biswal, Siddharth and Malin, Bradley and Duke, Jon and Stewart, Walter F. and Sun, Jimeng},
	urldate = {2018-03-02},
	date = {2017-03-19},
	year = {2017},
	eprinttype = {arxiv},
	eprint = {1703.06490},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1703.06490 PDF:/Users/seth/Zotero/storage/DH6N6TEY/Choi et al. - 2017 - Generating Multi-label Discrete Patient Records us.pdf:application/pdf;arXiv.org Snapshot:/Users/seth/Zotero/storage/CM43RJTF/1703.html:text/html}
}

@article{miotto_deep_2016,
	title = {Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records},
	volume = {6},
	rights = {2016 Nature Publishing Group},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep26094},
	doi = {10.1038/srep26094},
	shorttitle = {Deep Patient},
	abstract = {Secondary use of electronic health records ({EHRs}) promises to advance clinical research and better inform clinical decision making. Challenges in summarizing and representing patient data prevent widespread practice of predictive modeling using {EHRs}. Here we present a novel unsupervised deep feature learning method to derive a general-purpose patient representation from {EHR} data that facilitates clinical predictive modeling. In particular, a three-layer stack of denoising autoencoders was used to capture hierarchical regularities and dependencies in the aggregated {EHRs} of about 700,000 patients from the Mount Sinai data warehouse. The result is a representation we name “deep patient”. We evaluated this representation as broadly predictive of health states by assessing the probability of patients to develop various diseases. We performed evaluation using 76,214 test patients comprising 78 diseases from diverse clinical domains and temporal windows. Our results significantly outperformed those achieved using representations based on raw {EHR} data and alternative feature learning strategies. Prediction performance for severe diabetes, schizophrenia, and various cancers were among the top performing. These findings indicate that deep learning applied to {EHRs} can derive patient representations that offer improved clinical predictions, and could provide a machine learning framework for augmenting clinical decision systems.},
	pages = {26094},
	journaltitle = {Scientific Reports},
	author = {Miotto, Riccardo and Li, Li and Kidd, Brian A. and Dudley, Joel T.},
	urldate = {2018-03-02},
	date = {2016-05-17},
	langid = {english},
	file = {Full Text PDF:/Users/seth/Zotero/storage/AVAW6XU9/Miotto et al. - 2016 - Deep Patient An Unsupervised Representation to Pr.pdf:application/pdf;Snapshot:/Users/seth/Zotero/storage/SEJBDFSX/srep26094.html:text/html}
}


@article{walonoski_synthea:_2018,
	title = {Synthea: {An} approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record},
	volume = {25},
	issn = {1067-5027},
	shorttitle = {Synthea},
	url = {https://academic.oup.com/jamia/article/25/3/230/4098271},
	doi = {10.1093/jamia/ocx079},
	abstract = {ObjectiveOur objective is to create a source of synthetic electronic health records that is readily available; suited to industrial, innovation, research, and educational uses; and free of legal, privacy, security, and intellectual property restrictions.Materials and MethodsWe developed Synthea, an open-source software package that simulates the lifespans of synthetic patients, modeling the 10 most frequent reasons for primary care encounters and the 10 chronic conditions with the highest morbidity in the United States.ResultsSynthea adheres to a previously developed conceptual framework, scales via open-source deployment on the Internet, and may be extended with additional disease and treatment modules developed by its user community. One million synthetic patient records are now freely available online, encoded in standard formats (eg, Health Level-7 [HL7] Fast Healthcare Interoperability Resources [FHIR] and Consolidated-Clinical Document Architecture), and accessible through an HL7 FHIR application program interface.DiscussionHealth care lags other industries in information technology, data exchange, and interoperability. The lack of freely distributable health records has long hindered innovation in health care. Approaches and tools are available to inexpensively generate synthetic health records at scale without accidental disclosure risk, lowering current barriers to entry for promising early-stage developments. By engaging a growing community of users, the synthetic data generated will become increasingly comprehensive, detailed, and realistic over time.ConclusionSynthetic patients can be simulated with models of disease progression and corresponding standards of care to produce risk-free realistic synthetic health care records at scale.},
	language = {en},
	number = {3},
	urldate = {2018-03-02},
	journal = {Journal of the American Medical Informatics Association},
	author = {Walonoski, Jason and Kramer, Mark and Nichols, Joseph and Quina, Andre and Moesel, Chris and Hall, Dylan and Duffett, Carlton and Dube, Kudakwashe and Gallagher, Thomas and McLachlan, Scott},
	month = mar,
	year = {2018},
	pages = {230--238},
	file = {Full Text PDF:/Users/seth/Zotero/storage/SD9XE2BJ/Walonoski et al. - 2018 - Synthea An approach, method, and software mechani.pdf:application/pdf}
}

@inproceedings{gray_quickly_1994,
	address = {New York, NY, USA},
	series = {{SIGMOD} '94},
	title = {Quickly {Generating} {Billion}-record {Synthetic} {Databases}},
	isbn = {978-0-89791-639-4},
	url = {http://doi.acm.org/10.1145/191839.191886},
	doi = {10.1145/191839.191886},
	abstract = {Evaluating database system performance often requires generating synthetic databases—ones having certain statistical properties but filled with dummy information. When evaluating different database designs, it is often necessary to generate several databases and evaluate each design. As database sizes grow to terabytes, generation often takes longer than evaluation. This paper presents several database generation techniques. In particular it discusses: (1) Parallelism to get generation speedup and scaleup. (2) Congruential generators to get dense unique uniform distributions. (3) Special-case discrete logarithms to generate indices concurrent to the base table generation. (4) Modification of (2) to get exponential, normal, and self-similar distributions.The discussion is in terms of generating billion-record SQL databases using C programs running on a shared-nothing computer system consisting of a hundred processors, with a thousand discs. The ideas apply to smaller databases, but large databases present the more difficult problems.},
	urldate = {2018-05-09},
	booktitle = {Proceedings of the 1994 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Gray, Jim and Sundaresan, Prakash and Englert, Susanne and Baclawski, Ken and Weinberger, Peter J.},
	year = {1994},
	pages = {243--252},
	file = {ACM Full Text PDF:/Users/seth/Zotero/storage/AGN33HBZ/Gray et al. - 1994 - Quickly Generating Billion-record Synthetic Databa.pdf:application/pdf}
}

@article{sweeney_2002,
	title = {k-{ANONYMITY}: {A} {MODEL} {FOR} {PROTECTING} {PRIVACY}},
	volume = {10},
	issn = {0218-4885},
	shorttitle = {k-{ANONYMITY}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218488502001648},
	doi = {10.1142/S0218488502001648},
	abstract = {Consider a data holder, such as a hospital or a bank, that has a privately held  collection of person-specific, field structured data. Suppose the data holder  wants to share a version of the data with researchers. How can a data holder release  a version of its private data with scientific guarantees that the individuals who are  the subjects of the data cannot be re-identified while the data remain  practically useful? The solution provided in this paper includes a formal  protection model named k-anonymity and a set of accompanying policies  for deployment. A release provides k-anonymity protection if the  information for each person contained in the release cannot be distinguished from  at least k-1 individuals whose information also appears in the release.  This paper also examines re-identification attacks that can be realized on  releases that adhere to k-anonymity unless accompanying policies  are respected. The k-anonymity protection model is important because  it forms the basis on which the real-world systems known as Datafly,  μ-Argus and k-Similar provide guarantees of privacy  protection.},
	number = {05},
	urldate = {2018-05-09},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Sweeney, Latanya},
	month = oct,
	year = {2002},
	pages = {557--570},
	file = {Snapshot:/Users/seth/Zotero/storage/MGW5P3BG/S0218488502001648.html:text/html}
}

@article{gal_data_2014,
	title = {A data recipient centered de-identification method to retain statistical attributes},
	volume = {50},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046414000021},
	doi = {10.1016/j.jbi.2014.01.001},
	abstract = {Privacy has always been a great concern of patients and medical service providers. As a result of the recent advances in information technology and th…},
	language = {en},
	urldate = {2018-05-07},
	journal = {Journal of Biomedical Informatics},
	author = {Gal, Tamas S. and Tucker, Thomas C. and Gangopadhyay, Aryya and Chen, Zhiyuan},
	month = aug,
	year = {2014},
	pages = {32--45},
	file = {2014 - A data recipient centered de-identification method.pdf:/Users/seth/Zotero/storage/88HPIE4B/2014 - A data recipient centered de-identification method.pdf:application/pdf}
}

@article{aggarwal_static_2008,
	title = {On {Static} and {Dynamic} {Methods} for {Condensation}-based {Privacy}-preserving {Data} {Mining}},
	volume = {33},
	issn = {0362-5915},
	url = {http://doi.acm.org/10.1145/1331904.1331906},
	doi = {10.1145/1331904.1331906},
	abstract = {In recent years, privacy-preserving data mining has become an important problem because of the large amount of personal data which is tracked by many business applications. In many cases, users are unwilling to provide personal information unless the privacy of sensitive information is guaranteed. In this paper, we propose a new framework for privacy-preserving data mining of multidimensional data. Previous work for privacy-preserving data mining uses a perturbation approach which reconstructs data distributions in order to perform the mining. Such an approach treats each dimension independently and therefore ignores the correlations between the different dimensions. In addition, it requires the development of a new distribution-based algorithm for each data mining problem, since it does not use the multidimensional records, but uses aggregate distributions of the data as input. This leads to a fundamental re-design of data mining algorithms. In this paper, we will develop a new and flexible approach for privacy-preserving data mining that does not require new problem-specific algorithms, since it maps the original data set into a new anonymized data set. These anonymized data closely match the characteristics of the original data including the correlations among the different dimensions. We will show how to extend the method to the case of data streams. We present empirical results illustrating the effectiveness of the method. We also show the efficiency of the method for data streams.},
	number = {1},
	urldate = {2018-05-09},
	journal = {ACM Trans. Database Syst.},
	author = {Aggarwal, Charu C. and Yu, Philip S.},
	month = mar,
	year = {2008},
	keywords = {\textit{k}-anonymity, databases data mining, Privacy},
	pages = {2:1--2:39},
	file = {ACM Full Text PDF:/Users/seth/Zotero/storage/25ACYH3L/Aggarwal and Yu - 2008 - On Static and Dynamic Methods for Condensation-bas.pdf:application/pdf}
}

@inproceedings{drummond_replicability_2009,
	address = {Montreal, Canada},
	title = {Replicability is not {Reproducibility}: {Nor} is it {Good} {Science}},
	url = {http://www.csi.uottawa.ca/~cdrummon/pubs/ICMLws09.pdf},
	abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our ﬁeld. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientiﬁc discipline, being able to replicate experiments is paramount. I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important diﬀerences between the two. Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.},
	language = {en},
	booktitle = {Proc. of the {Evaluation} {Methods} for {Machine} {Learning} {Workshop} at the 26th {ICML}},
	author = {Drummond, Chris},
	year = {2009},
	pages = {4},
	file = {Drummond - Replicability is not Reproducibility Nor is it Go.pdf:/Users/seth/Zotero/storage/UJKFVDB7/Drummond - Replicability is not Reproducibility Nor is it Go.pdf:application/pdf}
}

@article{leek_opinion_2015,
	title = {Opinion: {Reproducible} research can still be wrong: {Adopting} a prevention approach},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Opinion},
	url = {http://www.pnas.org/content/112/6/1645},
	doi = {10.1073/pnas.1421412111},
	abstract = {National Academy of Sciences},
	language = {en},
	number = {6},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Leek, Jeffrey T. and Peng, Roger D.},
	month = feb,
	year = {2015},
	pmid = {25670866},
	pages = {1645--1646},
	file = {PNAS-2015-Leek-1645-6.pdf:/Users/seth/Zotero/storage/NYUHA9SU/PNAS-2015-Leek-1645-6.pdf:application/pdf}
}

@article{peng_reproducible_2011,
	title = {Reproducible {Research} in {Computational} {Science}},
	volume = {334},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3383002/},
	doi = {10.1126/science.1213847},
	abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
	number = {6060},
	urldate = {2018-05-09},
	journal = {Science (New York, N.y.)},
	author = {Peng, Roger D.},
	month = dec,
	year = {2011},
	pmid = {22144613},
	pmcid = {PMC3383002},
	pages = {1226--1227},
	file = {PubMed Central Full Text PDF:/Users/seth/Zotero/storage/MGTXGWX5/Peng - 2011 - Reproducible Research in Computational Science.pdf:application/pdf}
}

@article{peng_reproducible_2006,
	title = {Reproducible {Epidemiologic} {Research}},
	volume = {163},
	issn = {0002-9262},
	url = {https://academic.oup.com/aje/article/163/9/783/108733},
	doi = {10.1093/aje/kwj093},
	abstract = {Abstract.  The replication of important findings by multiple independent investigators is fundamental to the accumulation of scientific evidence. Researchers in},
	language = {en},
	number = {9},
	urldate = {2018-05-10},
	journal = {American Journal of Epidemiology},
	author = {Peng, Roger D. and Dominici, Francesca and Zeger, Scott L.},
	month = may,
	year = {2006},
	pages = {783--789},
	file = {Full Text PDF:/Users/seth/Zotero/storage/J7WZBEHP/Peng et al. - 2006 - Reproducible Epidemiologic Research.pdf:application/pdf;Snapshot:/Users/seth/Zotero/storage/8C8MW8VC/108733.html:text/html}
}

@article{schwab_making_2000,
	title = {Making {Scientific} {Computations} {Reproducible}},
	volume = {2},
	issn = {1521-9615},
	url = {doi.ieeecomputersociety.org/10.1109/5992.881708},
	abstract = {To verify a research paper's computational results, readers typically have to recreate them from scratch. ReDoc is a simple software filing system for authors that lets readers easily reproduce computational results using standardized rules and commands.},
	number = {6},
	urldate = {2018-05-10},
	journal = {Computing in Science \& Engineering},
	author = {Schwab, M. and Karrenbach, M. and Claerbout, J.},
	year = {2000},
	pages = {61--67},
	file = {Schwab et al. - 2000 - Making Scientific Computations Reproducible.pdf:/Users/seth/Zotero/storage/VCVXWBWH/Schwab et al. - 2000 - Making Scientific Computations Reproducible.pdf:application/pdf;Snapshot:/Users/seth/Zotero/storage/TY26S8RJ/c6061-abs.html:text/html}
}

@inproceedings{ping17datasynthesizer,
  author = {Ping, Haoyue and Stoyanovich, Julia and Howe, Bill},
  title = {DataSynthesizer: Privacy-Preserving Synthetic Datasets},
  booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
  series = {SSDBM '17},
  year = {2017},
  isbn = {978-1-4503-5282-6},
  location = {Chicago, IL, USA},
  pages = {42:1--42:5},
  articleno = {42},
  numpages = {5},
  url = {http://doi.acm.org/10.1145/3085504.3091117},
  doi = {10.1145/3085504.3091117},
  acmid = {3091117},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {Data Sharing, Differential Privacy, Synthetic Data}
}

@article{Bindschaedler2017,
 author = {Bindschaedler, Vincent and Shokri, Reza and Gunter, Carl A.},
 title = {Plausible Deniability for Privacy-preserving Data Synthesis},
 journal = {Proc. VLDB Endow.},
 issue_date = {January 2017},
 volume = {10},
 number = {5},
 month = jan,
 year = {2017},
 issn = {2150-8097},
 pages = {481--492},
 numpages = {12},
 url = {https://doi.org/10.14778/3055540.3055542},
 doi = {10.14778/3055540.3055542},
 acmid = {3055542},
 publisher = {VLDB Endowment},
} 

@book{silverman_density_1986,
	address = {London ; New York},
	series = {Monographs on statistics and applied probability},
	title = {Density estimation for statistics and data analysis},
	isbn = {978-0-412-24620-3},
	number = {26},
	publisher = {Chapman and Hall},
	author = {Silverman, B. W.},
	year = {1986},
	keywords = {Estimation theory}
}

@article{howe_synthetic_2017,
	title = {Synthetic {Data} for {Social} {Good}},
	url = {http://arxiv.org/abs/1710.08874},
	abstract = {Data for good implies unfettered access to data. But data owners must be conservative about how, when, and why they share data or risk violating the trust of the people they aim to help, losing their funding, or breaking the law. Data sharing agreements can help prevent privacy violations, but require a level of specificity that is premature during preliminary discussions, and can take over a year to establish. We consider the generation and use of synthetic data to facilitate ad hoc collaborations involving sensitive data. A good synthetic dataset has two properties: it is representative of the original data, and it provides strong guarantees about privacy. In this paper, we discuss important use cases for synthetic data that challenge the state of the art in privacy-preserving data generation, and describe DataSynthesizer, a dataset generation tool that takes a sensitive dataset as input and generates a structurally and statistically similar synthetic dataset, with strong privacy guarantees, as output. The data owners need not release their data, while potential collaborators can begin developing models and methods with some confidence that their results will work similarly on the real dataset. The distinguishing feature of DataSynthesizer is its usability - in most cases, the data owner need not specify any parameters to start generating and sharing data safely and effectively. The code implementing DataSynthesizer is publicly available on GitHub at https://github.com/DataResponsibly. The work on DataSynthesizer is part of the Data, Responsibly project, where the goal is to operationalize responsibility in data sharing, integration, analysis and use.},
	urldate = {2018-05-14},
	journal = {arXiv:1710.08874 [cs]},
	author = {Howe, Bill and Stoyanovich, Julia and Ping, Haoyue and Herman, Bernease and Gee, Matt},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.08874},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv\:1710.08874 PDF:/Users/seth/Zotero/storage/BTR7AW3J/Howe et al. - 2017 - Synthetic Data for Social Good.pdf:application/pdf}
}

@misc{naep_2009,
	title = {{NAEP} {Analysis} and {Scaling} - {Minimum} {School} and {Student} {Sample} {Sizes} for {Reporting} {Group} {Results}},
	url = {https://nces.ed.gov/nationsreportcard/tdw/analysis/summary_rules_minimum.aspx},
	abstract = {Enter overall description here.  Each page however should have individual descriptions.},
	language = {EN},
	author = {{National Center for Education Statistics}},
	month = sep,
	year = {2009},
	file = {Snapshot:/Users/seth/Zotero/storage/D8ZKY2E7/summary_rules_minimum.html:text/html}
}

http://www.shokri.org/files/Shokri-CCS2015.pdf
get citation for Pandas