"present papers from the coding perspective so that reproducibility and replication of results in the Machine Learning community becomes easier"


Need some information (and reference(s)) about IRBs and human subjects research?

Re-identification attacks - should we include brief description about this? Some references: 
* El Emam K, Jonker E, Arbuckle L, Malin B. A systematic review of re-identification attacks on health data. PLoS ONE 2011;6(12) [e28071, 12].
* Sweeney L. k-anonymity: a model for protecting privacy. Ijufks 2002;10:557–70.

\section{Introduction}

Reproducing machine learning results in the field of health care is challenging. Historically, many published research articles do not make code nor data available which limits the ability of other researchers to validate or reproduce results. While some high impact journals such as the Nature Research journal family have adopted policies to promote software and data sharing \cite{natmethods}, not all journals and locations where machine learning methods are published have such policies. Despite a growing trend toward code and data sharing policies, there are additional difficulties in sharing health care data due to legal requirements. 

 In the U.S., the primary law affecting health data privacy is the Health Insurance Portability and Accountability Act (HIPAA) \cite{hippapro}. Additionally, organizations desiring to perform biomedical and social, behavioral, and educational research are subject to Common Rule (45 CFR 46, Subpart A) and/or the U.S. Food and Drug Administration’s regulations (21 CFR 50 and 56). Furthermore, the Common Rule established Institutional Review Boards which are responsible and the institutional level of safe guarding human subjects in any research; similar layers of laws and legal controls exist in other countries. Due to the many constraints in place, even appropriate and routine requests to access data can take a significant amount of effort and time. Importantly, only the approved group or individual can access data; it cannot be shared in original form or used for other purposes, so secondary, educational or other key data use cases are left unmet.
 
 As machine learning results are highly data dependent, the barriers around sharing health data make it difficult or impossible for others to verify or reproduce results with novel machine learning methods in the health care domain (? cite recent non-reproducible examples such as Google FHIR, others?). In order to resolve the problem of data sharing, various methods for data synthesis and de-identification have been developed in other data domains that have been applied to health care. The use of privacy preserving techniques as methods to safely share health care or other data have a history of re-identification risk as detailed by Sweeney \cite{sweeney_2002}. Alternative techniques of synthetic data have been used to avoid the potential problem of re-identification. One early example by Gray et al. \cite{gray_quickly_1994} used pseudo-random generation techniques to populate large databases with synthetic data for performance evaluation. Subsequent advances in techniques have generated more realistic data based on observed and publicly reported statistics, such as with the Synthea system \cite{walonoski_synthea:_2018}, that generates an entire outpatient Electronic Health Record (EHR) based on population statistics. 
 
 While advances in both de-identification and synthesis of data have been impressive, still more progress needs to be made. Techniques such as Synthea that generate an entire EHR require significant computational power and model development of every disease state desired and may not have the level of specificity as found in an actual EHR due to rare or complex conditions. Other techniques based on actual health data such as described by Gal et al. \cite{gal_data_2014} provide for the complexities found in real health data by building off of the condensation algorithm \cite{aggarwal_static_2008} though use of k-means clustering, Principle Component Analysis, and independent data generation pipelines based on a specific outcome variable. While the method as depicted by Gal results in a use case specific synthetic data set, the manual intervention required to develop a synthetic data requires an ongoing effort between the data source and researchers needing access to data. In this paper we propose a method that allows the user of the data to identify what attributes they are interested in, does not require manual intervention or generation of synthetic data, and is modular so that as new techniques are developed the system could be updated to take advantage of them.
 
 \section{Reproducibility}
 %Good discussion of what reproducibilty vs replicability means
 %http://languagelog.ldc.upenn.edu/nll/?p=21956
 To clarify what we mean by reproducibility in machine learning, a brief discussion of what the term reproducibility
 Over the past 15 years or so, there has been some significant papers clarifying the definition and importance of reproducibility, along with the closely related term replicability. Leek and Peng have defined reproducibility as the "ability to recompute data analytic results given an observed dataset and knowledge of the data analysis pipeline," and replicability as "the chance that an independent experi- ment targeting the same scientific question will produce a consistent result" \cite{leek_opinion_2015}. One early influential paper on the topic of reproducibility in scientifici computing by Schwab details key factors in reproducbility as: having data, input parameters, documenation, software code, and an environment capable of running the provided software code \cite{schwab_making_2000}.
 
 