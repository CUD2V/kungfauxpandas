\documentclass{article}


%\usepackage{nips_2017}
\usepackage[final]{nips_2017}
% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{soul}           % added by seth - remove before submission, provides highlighting support

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
 
\urlstyle{same}
\title{%
  Kung Faux Pandas \\
  \large A tool for enabling data sharing in health care}

\author{
  James King \& Seth Russell\\
  Data Science to Patient Value (D2V)\\
  School of Medicine \\
  University of Colorado Anschutz Medical Campus\\
  Aurora, CO\\
  \texttt{james.king@ucdenver.edu} \\
  \texttt{seth.russell@ucdenver.edu} \\
  }

\date{May 2018}

\begin{document}

% Advice from Tell:  articulate the problem with evidence (i.e. papers)
%   Enable reproducibility - patient value, make process of replication easier
%   Time saving
%   legal risk (carrying data in laptops, etc)
%   Multiple comparison?
%   Educational data sets
%  Definition of :reproducibility 
%       reproducibilty - everything same - code, data, result
%       replicability - code same, new data, same result find Peng & Leek reference (also look at blog)
%   unit testing...

\maketitle

\begin{abstract}
\hl{to finish:} A description of an end-to-end system for easily generating synthetic data which contains no Personal Health Information (PHI), allowing example data to be distributed for reproducibility testing and other purposes.  

\end{abstract}



\section{Introduction}

Reproducing machine learning results in the field of health care is challenging. Many published research articles do not make code nor data available. Journals could require the inclusion of code in submissions, however most health care data is very strictly protected by law \cite{hippapro}.  

Various approaches to building data sets which simultaneously have scientific utility and comply with privacy laws.  Broadly speaking, these methods fall into two categories: anonymization and synthesis.  Anonymization methods attempt to keep as much of the "real" data as possible, whilst ensuring that person-specific data 

The unique contribution to the space of reproducible machine learning by this paper is a no-configuration required method for faux data creation from user generated queries.The faux data will resemble the original data set but can be used freely without compliance risk because it contains no Protected Health Information (PHI).  Furthermore, with this method it is possible for an investigator to perform hypothesis articulation, data cleaning, analysis, visualization, and documentation without the ever having access to the underlying PHI data.

\section{Background}

Over the past 18 years, there has been several significant papers clarifying the definition and importance of reproducibility along with the closely related term replicability. Leek and Peng have defined reproducibility as the "ability to recompute data analytic results given an observed dataset and knowledge of the data analysis pipeline," and replicability as "the chance that an independent experiment targeting the same scientific question will produce a consistent result" \cite{leek_opinion_2015}; others such as Drummond use the terms in a reverse fashion \cite{drummond_replicability_2009}. Despite the semantic disagreement, both groups agree that an independent experiment with confirmatory results is the strongest support of any experiment. One early influential paper on the topic of reproducibility in scientific computing details key factors in reproducbility as having: data, input parameters, documenation, software code, and an environment capable of running the provided software code; items that cannot easily nor clearly be communicated by paper \cite{schwab_making_2000}. Reproducibility should be treated as a minimium required standard that all published research should meet \cite{peng_reproducible_2006}. Specifically in the context of machine learning, this means including details to a level someone else could create the same environment including hardware configuration and run times, data used for all experiments, source code, documentation on data and how to run/configure software, and tests that verify the software runs correctly. The last item is important but often overlooked - unit tests can show how a researcher validated their code was giving expected results which can often lead to insights not normally communicated in a results section of a journal article. Once a result can be reproduced, new researchers can then build upon the methods, gather new data for testing/validation, or discover alternative methods to replicate a result.

As machine learning results are highly data dependent, the barriers around sharing health data make it difficult or impossible for others to verify or reproduce results with novel machine learning methods in the health care domain (? cite recent difficult to reproduce examples such as Google FHIR paper, others?). In order to resolve the problem of data sharing, various methods for data synthesis and de-identification have been developed in other data domains that have been applied to health care. The use of privacy preserving techniques as methods to safely share health care or other data have a history of re-identification risk as detailed by Sweeney \cite{sweeney_2002}. Alternative techniques of synthetic data have been used to avoid the potential problem of re-identification. One early example by Gray et al. \cite{gray_quickly_1994} used pseudo-random generation techniques to populate large databases with synthetic data for performance evaluation. Subsequent advances in techniques have generated more realistic data based on observed and publicly reported statistics, such as with the Synthea system \cite{walonoski_synthea:_2018}, that generates an entire outpatient Electronic Health Record (EHR) based on population statistics.

While advances in both de-identification and synthesis of data have been impressive, still more progress needs to be made. Techniques such as Synthea that generate an entire EHR require significant computational power and model development of every disease state desired and may not have the level of specificity as found in an actual EHR due to rare or complex conditions. Other techniques based on actual health data such as described by Gal et al. \cite{gal_data_2014} provide for the complexities found in real health data by building off of the condensation algorithm \cite{aggarwal_static_2008} though use of k-means clustering, Principle Component Analysis, and independent data generation pipelines based on a specific outcome variable. While the method as depicted by Gal results in a use case specific synthetic data set, the manual intervention required to develop a synthetic data requires an ongoing effort between the data source and researchers needing access to data. In this paper we propose a method that allows the user of the data to identify what attributes they are interested in, does not require manual intervention or generation of synthetic data, and is modular so that as new techniques are developed the system could be updated to take advantage of them.

\section{Methods}

? James write this section ?

\section{Conclusion}

Basically need to put abstract here...

\section{Appendix: Legal References}

\hl{Where to put this? Probably don't want to waste any of our 4 pages with an appendix... Include this in body or not at all?}


In the U.S., the primary law affecting health data privacy is the Health Insurance Portability and Accountability Act (\href{https://www.hhs.gov/hipaa/for-professionals/index.html}{HIPAA}). 


Additionally, organizations desiring to perform biomedical and social, behavioral, and educational research are subject to Common Rule (\href{https://www.hhs.gov/ohrp/regulations-and-policy/regulations/45-cfr-46/index.html#subparta}{45 CFR 46, Subpart A}) and/or the U.S. Food and Drug Administrationâ€™s regulations (\href{https://www.ecfr.gov/cgi-bin/text-idx?SID=faa4b2b2900a70fbcac4a773c9da0f0f&mc=true&node=pt21.1.50&rgn=div5}{21 CFR 50} and \href{https://www.ecfr.gov/cgi-bin/text-idx?SID=faa4b2b2900a70fbcac4a773c9da0f0f&mc=true&node=pt21.1.56&rgn=div5}{56}). Furthermore, the Common Rule established Institutional Review Boards which are responsible and the institutional level of safe guarding human subjects in any research. 


\bibliographystyle{unsrt}
\bibliography{bib}

\end{document}